# Enterprise Knowledge Assistant

ğŸš€ **Production-grade intelligent assistant for enterprise document analysis with Agentic RAG and MLOps**

## Overview

A sophisticated multi-agent RAG (Retrieval-Augmented Generation) system designed for legal/compliance teams to query company policies, contracts, and regulations with high accuracy and explainability.

### Key Features

- **ğŸ” Three-Way Hybrid Retrieval**: Combines BM25, Dense Vectors (BGE-M3), and SPLADE sparse vectors with ColBERT reranking
- **ğŸ¤– Multi-Agent Architecture**: LangGraph-based orchestration with specialized agents for planning, extraction, QA, and validation
- **ğŸ“Š Knowledge Graph Integration**: Neo4j for structured knowledge representation and multi-hop reasoning
- **ğŸ”§ Configurable LLM Client**: Works with any OpenAI-compatible API (OpenAI, Azure, Ollama, vLLM, Anthropic, etc.)
- **ğŸ“ˆ Full MLOps Pipeline**: DVC, MLflow, Airflow, RAGAS evaluation, automated retraining
- **ğŸ” Enterprise Security**: JWT auth, RBAC, rate limiting, PII masking
- **ğŸ“‰ Comprehensive Monitoring**: Prometheus, Grafana, distributed tracing

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API Gateway (FastAPI)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Multi-Agent Orchestrator                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚
â”‚  â”‚  â”‚ Planner â”‚ â”‚Extractorâ”‚ â”‚   QA    â”‚ â”‚Validatorâ”‚        â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Hybrid Retrieval Pipeline                     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚  â”‚
â”‚  â”‚  â”‚ BM25 â”‚  â”‚Dense Vec â”‚  â”‚SPLADEâ”‚  â”‚ ColBERT â”‚           â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â”‚Reranker â”‚           â”‚  â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  â”‚
â”‚  â”‚         RRF Fusion (k=60)                                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    Data Layer                              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚  â”‚
â”‚  â”‚  â”‚  Qdrant  â”‚  â”‚  Neo4j   â”‚  â”‚  Redis   â”‚                â”‚  â”‚
â”‚  â”‚  â”‚(Vectors) â”‚  â”‚  (Graph) â”‚  â”‚ (Cache)  â”‚                â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites

- Python 3.11+
- Docker & Docker Compose
- CUDA-compatible GPU (recommended for embeddings)

### Installation

```bash
# Clone the repository
git clone https://github.com/your-org/enterprise-knowledge-assistant.git
cd enterprise-knowledge-assistant

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# or
.venv\Scripts\activate  # Windows

# Install dependencies
pip install -e ".[all]"

# Copy environment template
cp .env.example .env
# Edit .env with your configuration
```

### Start Infrastructure

```bash
# Start Qdrant, Neo4j, Redis
docker-compose up -d

# Verify services are running
docker-compose ps
```

### Run the API

```bash
# Development mode
uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000

# Production mode
uvicorn src.api.main:app --workers 4 --host 0.0.0.0 --port 8000
```

### API Documentation

Once running, access:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

## LLM Configuration

The system supports any LLM with an OpenAI-compatible API:

### OpenAI
```env
LLM_PROVIDER=openai
LLM_API_BASE=https://api.openai.com/v1
LLM_API_KEY=sk-...
LLM_MODEL=gpt-4-turbo
```

### Azure OpenAI
```env
LLM_PROVIDER=azure
LLM_API_BASE=https://your-resource.openai.azure.com
LLM_API_KEY=your-key
LLM_MODEL=your-deployment
LLM_API_VERSION=2024-02-15-preview
```

### Local Ollama
```env
LLM_PROVIDER=ollama
LLM_API_BASE=http://localhost:11434/v1
LLM_API_KEY=ollama
LLM_MODEL=llama3.1:70b
```

### vLLM / Custom
```env
LLM_PROVIDER=custom
LLM_API_BASE=http://localhost:8000/v1
LLM_API_KEY=not-needed
LLM_MODEL=meta-llama/Meta-Llama-3.1-70B-Instruct
```

## Project Structure

```
enterprise-knowledge-assistant/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/           # Multi-agent system
â”‚   â”œâ”€â”€ api/              # FastAPI application
â”‚   â”œâ”€â”€ models/           # LLM & embedding clients
â”‚   â”œâ”€â”€ retrieval/        # Hybrid retrieval pipeline
â”‚   â”œâ”€â”€ pipeline/         # Document processing
â”‚   â””â”€â”€ monitoring/       # Metrics & tracing
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ docker/           # Docker configurations
â”‚   â”œâ”€â”€ kubernetes/       # K8s manifests
â”‚   â””â”€â”€ terraform/        # IaC for AWS
â”œâ”€â”€ mlops/
â”‚   â”œâ”€â”€ airflow/          # DAGs for pipelines
â”‚   â”œâ”€â”€ mlflow/           # Experiment tracking
â”‚   â””â”€â”€ dvc/              # Data versioning
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus/       # Metrics collection
â”‚   â””â”€â”€ grafana/          # Dashboards
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â””â”€â”€ docs/
```

## Evaluation Metrics

| Metric | Target | Description |
|--------|--------|-------------|
| Context Precision | â‰¥ 0.85 | Retrieved context relevance |
| Context Recall | â‰¥ 0.90 | Coverage of relevant information |
| Faithfulness | â‰¥ 0.95 | Answer grounded in context |
| Answer Relevance | â‰¥ 0.88 | Response quality |
| P95 Latency | < 3s | End-to-end response time |
| Availability | 99.9% | System uptime |

## Development

```bash
# Run tests
pytest tests/ -v

# Run linting
ruff check src/

# Run type checking
mypy src/

# Run all checks
pre-commit run --all-files
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and linting
5. Submit a pull request

## License

MIT License - see [LICENSE](LICENSE) for details.
